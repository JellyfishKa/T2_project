# Бенчмарк LLM

Скрипт `llm_benchmark.py` гоняет запросы к моделям и собирает метрики: время ответа, оценка качества, доля успешных ответов, ориентировочная стоимость. Используются две HF-модели (Qwen, Llama) или клиенты backend (флаг `--backend`). Backend не изменяется.

---

## Метрики

| Метрика | Описание |
|--------|----------|
| **response_time_ms** | Среднее время ответа, мс |
| **quality_score** | Эвристическая оценка 0–100: длина, ключевые слова, структура, релевантность |
| **success_rate** | Доля успешных ответов, % |
| **cost_rub** | Ориентировочная стоимость в рублях (для HF-режима; для backend пока 0) |

Формула качества зашита в `_quality_score()`: баллы за длину, совпадение ожидаемых ключевых слов, наличие структуры (списки, цифры), отсутствие явных ошибок, релевантность домену (магазины, маршруты, доставка).

---

## Тестовые данные

В скрипте заданы точки (магазины с координатами в Москве) и пять типов задач: планирование маршрута, информация о магазинах, логистика доставки, анализ покрытия, оптимизация размещения. Промпты и ожидаемые ключевые слова лежат в `TEST_PROMPTS` и `TEST_LOCATIONS`.

---

## Запуск

Из корня репозитория (рекомендуется):

```bash
python ml/benchmarks/llm_benchmark.py
python ml/benchmarks/llm_benchmark.py --iterations 3
python ml/benchmarks/llm_benchmark.py --mock
python ml/benchmarks/llm_benchmark.py --backend --iterations 2
```

- **--iterations N** — сколько раз на модель прогоняется полный набор промптов (по умолчанию 5).
- **--mock** — модели не загружаются, ответы из заглушек; удобно для проверки пайплайна.
- **--backend** — бенчмарк по клиентам backend (Qwen, Llama); нужна доступная папка `backend/`.

Через скрипты из `ml/benchmarks/`: `run_benchmark.ps1`, `run_benchmark.bat` (ожидают venv в корне, например `ml_env`).

---

## Результаты и логи

- **results.json** — в той же папке, что и скрипт (`ml/benchmarks/results.json`). Внутри: timestamp, num_iterations, test_data_count, по каждой модели — model_name, model_id, use_mock, iterations (сырые прогоны), metrics (сводка).
- **benchmark.log** — детальный лог в той же папке.

## Анализ и отчёт (ML-3)

**Сравнение оптимизации (ML-5):** для сравнения результатов оптимизации по моделям и генерации отчёта:

```bash
python ml/benchmarks/optimization_comparison.py
```

Скрипт вызывает `compare_models_optimization(test_locations)`, сохраняет `optimization_results.json` и генерирует `optimization_report.md` (таблица: модель, расстояние, время, качество, время ответа, стоимость). По умолчанию используется mock (без вызова моделей); для реальных вызовов передайте `use_mock=False` в коде.

После прогона бенчмарка сформировать отчёт с рекомендациями Primary/Fallback/Tertiary:

```bash
python ml/benchmarks/generate_analysis_report.py
```

Скрипт читает `results.json`, строит сравнительную таблицу, ASCII-графики по времени, качеству и стоимости, выбирает по метрикам Primary/Fallback/Tertiary и записывает **analysis_report.md**. В отчёте также: когда переключаться на fallback, edge cases (падения, таймауты, деградация), рекомендации для настройки backend.

Ориентир по времени: mock — секунды; реальные модели — десятки минут на 5 итераций в зависимости от железа.

---

## Ошибки и fallback

При недоступности модели (сеть, память, отсутствие transformers) для HF-режима включается mock: бенчмарк не падает, в логах и результатах видно `use_mock: true`. Для `--backend` при неудачном импорте клиентов скрипт завершается с сообщением; сам backend не меняем.
